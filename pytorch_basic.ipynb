{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DB7zU4QnoSc"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial Used: https://medium.com/@ninads79shukla/pytorch-basics-3deffbebb2bd"
      ],
      "metadata": {
        "id": "EdGZ_UOj0hyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors"
      ],
      "metadata": {
        "id": "yU999K4boWOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor from a list\n",
        "tensor_from_list = torch.tensor([2, 3, 4, 5])\n",
        "print(\"Tensor from list:\", tensor_from_list)\n",
        "\n",
        "# Create a tensor with random values\n",
        "random_tensor = torch.rand((2, 3))\n",
        "print(\"Random tensor:\\n\", random_tensor)\n",
        "\n",
        "# Create a tensor filled with zeros\n",
        "zero_tensor = torch.zeros((3, 2))\n",
        "print(\"Zero tensor:\\n\", zero_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AVJlEbnnvTY",
        "outputId": "81cb3766-fa07-4e6d-d55c-c7902be1f2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from list: tensor([2, 3, 4, 5])\n",
            "Random tensor:\n",
            " tensor([[0.6010, 0.4378, 0.3448],\n",
            "        [0.9483, 0.8716, 0.2833]])\n",
            "Zero tensor:\n",
            " tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two tensors\n",
        "a = torch.tensor([1, 2, 3]) # create tensor from list\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Element-wise addition: add_zip\n",
        "c = a + b\n",
        "print(\"Element-wise addition:\", c)\n",
        "\n",
        "# Element-wise multiplication: mul_zip\n",
        "d = a * b\n",
        "print(\"Element-wise multiplication:\", d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrvmrWH2n9aX",
        "outputId": "9d5e36b6-bb26-4402-abd9-00d7ec42a63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element-wise addition: tensor([5, 7, 9])\n",
            "Element-wise multiplication: tensor([ 4, 10, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two matrices\n",
        "matrix1 = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix2 = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# Matrix multiplication\n",
        "matrix_product = torch.mm(matrix1, matrix2)\n",
        "print(\"Matrix multiplication:\\n\", matrix_product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaWxbLtoJZE",
        "outputId": "abb0c976-9f36-4fc4-b696-c7731d406399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multiplication:\n",
            " tensor([[19, 22],\n",
            "        [43, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tensor\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "\n",
        "# In-place addition: add_map\n",
        "tensor.add_(5)\n",
        "print(\"In-place addition:\", tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv6aq6SmoP-g",
        "outputId": "b4fdee64-986f-4978-e6bf-c878f557b374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-place addition: tensor([6, 7, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd (Auto-differentiation)\n",
        "\n",
        "Autograd allows you to automatically compute gradients for tensor operations. This is crucial for optimizing neural networks using gradient descent.\n",
        "\n",
        "We use gradient descent to optimize neural networks because it is an efficient and scalable algorithm for finding the set of weights and biases that minimize the networkâ€™s loss function"
      ],
      "metadata": {
        "id": "ZnB3PYzQofbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor with requires_grad=True\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "print(\"Tensor x:\", x)\n",
        "\n",
        "# Perform operations\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(\"Tensor y:\", y)\n",
        "print(\"Tensor z:\", z)\n",
        "print(\"Tensor out:\", out)\n",
        "\n",
        "# Compute gradients\n",
        "out.backward()\n",
        "\n",
        "# Print gradients\n",
        "print(\"Gradient of x:\", x.grad)\n",
        "print(\"Gradient of y:\", y.grad)\n",
        "# Only the leaf Tensor has the attribute .grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-BeIKnmokrb",
        "outputId": "acfcf161-7cce-4fbb-96d1-20ef683488d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor x: tensor([2., 3.], requires_grad=True)\n",
            "Tensor y: tensor([4., 5.], grad_fn=<AddBackward0>)\n",
            "Tensor z: tensor([48., 75.], grad_fn=<MulBackward0>)\n",
            "Tensor out: tensor(61.5000, grad_fn=<MeanBackward0>)\n",
            "Gradient of x: tensor([12., 15.])\n",
            "Gradient of y: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-844ef7b7d8d0>:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(\"Gradient of y:\", y.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation in PyTorch\n",
        "\n",
        "**Definition of Backpropagation Algorithm:**\n",
        "\n",
        "Backpropagation is the process of adjusting the weights of a neural network by analyzing the error rate from the previous iteration.\n",
        "\n",
        "\n",
        "**Backpropagation involves three main steps:**\n",
        "\n",
        "1. Forward Pass: Compute the output of the network.\n",
        "2. Compute Loss: Calculate the difference between the predicted output and the actual target.\n",
        "3. Backward Pass: Compute the gradient of the loss with respect to each parameter using the chain rule.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6LFFtydepehQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of Backpropagation - step by step"
      ],
      "metadata": {
        "id": "9EFFIShqr1AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 10)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(10, 1)  # Hidden layer to output layer\n",
        "        # so there is one hidden layer in this model, and in this hidden layer there are 10 neurons/splits\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNN()"
      ],
      "metadata": {
        "id": "rqK2yeIir2sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output of the Last Layer**\n",
        "\n",
        "\n",
        "1. Classification Tasks: For classification tasks, the type of activation function in the last layer depends on the number of classes:\n",
        "\n",
        "- Binary Classification (Two Classes): Use a sigmoid activation function in the last layer. It squashes the output between 0 and 1, which can be interpreted as a probability for class membership.\n",
        "- Multiclass Classification (More Than Two Classes): Use a softmax activation in the last layer to produce probabilities across multiple classes. Softmax converts the logits into a probability distribution where the sum of all output values equals 1.\n",
        "\n",
        "2. Regression Tasks: For regression tasks, where you are predicting continuous values (like predicting house prices), no activation function is typically used in the last layer. The output of the network is just the raw score or value from the final neuron(s).\n"
      ],
      "metadata": {
        "id": "CjXkbGtgtfEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the Loss Function and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "RJL50I1CtABc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Sample data: simple regression task\n",
        "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])"
      ],
      "metadata": {
        "id": "44rImgTDuWFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Forward pass: generate the output\n",
        "outputs = model(x_train)\n",
        "print(\"Model Outputs:\\n\", outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cucysto8ucG_",
        "outputId": "db0c6dc1-c425-465c-a0cf-fdefa44992b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Outputs:\n",
            " tensor([[-0.0695],\n",
            "        [-0.0823],\n",
            "        [-0.0950],\n",
            "        [-0.1078]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Compute loss\n",
        "loss = criterion(outputs, y_train)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR4wLScZulkv",
        "outputId": "973280d2-582d-4f38-b3bf-eab385b53633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 30.958513259887695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Backward pass: perform the backward pass to compute the gradients on parameters\n",
        "\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "Mcusib3-ut7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, the gradients of the loss with respect to each parameter are computed and stored in the .grad attribute of each parameter."
      ],
      "metadata": {
        "id": "Ze0kNyj-w6Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"Gradient of {name}:\\n\", param.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD0PAT0JwMeo",
        "outputId": "539e7272-25c7-4e64-a092-9f9d585bc7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of fc1.weight:\n",
            " tensor([[-0.9748],\n",
            "        [ 2.6472],\n",
            "        [-5.7999],\n",
            "        [ 0.0000],\n",
            "        [ 0.2926],\n",
            "        [ 0.0000],\n",
            "        [ 0.0000],\n",
            "        [ 0.0000],\n",
            "        [ 0.0000],\n",
            "        [ 0.0000]])\n",
            "Gradient of fc1.bias:\n",
            " tensor([-0.3255,  0.8840, -1.9369,  0.0000,  0.0977,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000])\n",
            "Gradient of fc2.weight:\n",
            " tensor([[-22.7762, -38.0141, -15.7886,   0.0000, -24.9799,   0.0000,   0.0000,\n",
            "           0.0000,   0.0000,   0.0000]])\n",
            "Gradient of fc2.bias:\n",
            " tensor([-10.1773])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Update Parameters: Use the optimizer to update the model parameters.\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "IUjL5USFwciy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer.step() is then called to update the parameters of the model. It adjusts the parameters based on their gradients and the optimization algorithm used (e.g., Stochastic Gradient Descent, Adam, etc.). The optimizer uses the gradients stored in each parameterâ€™s .grad attribute to update them according to its specific update rule."
      ],
      "metadata": {
        "id": "mThOyolpw-Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of Backpropagation - Full Training Loop\n",
        "\n",
        "The Process:\n",
        "\n",
        "1. Forward Pass: You compute the output of the model based on the current parameters.\n",
        "\n",
        "2. Loss Calculation: You calculate the loss (error) based on the output and the target.\n",
        "\n",
        "3. Backward Pass (Backpropagation): You call loss.backward() to compute the gradients of the loss with respect to each model parameter. These gradients are accumulated in the .grad attribute of each parameter.\n",
        "\n",
        "4. Parameter Update: You call optimizer.step() to update the modelâ€™s parameters based on the gradients stored in .grad.\n",
        "\n",
        "5. Zero the Gradients: Before the next iteration, you call optimizer.zero_grad() to clear the accumulated gradients so that new gradients can be calculated for the current batch of data."
      ],
      "metadata": {
        "id": "P7t6DGq0xSWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x_train)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXC0e0RVxaTZ",
        "outputId": "3d3a09ac-cc3e-4dd1-dbab-33d491fc50ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.0903\n",
            "Epoch [200/1000], Loss: 0.0201\n",
            "Epoch [300/1000], Loss: 0.0042\n",
            "Epoch [400/1000], Loss: 0.0008\n",
            "Epoch [500/1000], Loss: 0.0002\n",
            "Epoch [600/1000], Loss: 0.0000\n",
            "Epoch [700/1000], Loss: 0.0000\n",
            "Epoch [800/1000], Loss: 0.0000\n",
            "Epoch [900/1000], Loss: 0.0000\n",
            "Epoch [1000/1000], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Epoch**: One complete pass through the entire training dataset. For example, if your dataset has 10,000 samples, one epoch means the model has seen all 10,000 samples once.\n",
        "\n",
        "**Batch Size**: The number of training samples processed before the model's parameters are updated. Instead of using the entire dataset at once (which could be computationally expensive), the data is split into smaller groups called batches. Each batch is processed separately.\n",
        "\n",
        "**Iteration**: One iteration refers to one update of the modelâ€™s parameters, which happens after processing one batch of data.\n",
        "\n",
        "***number of iterations = number of samples/ batch size***"
      ],
      "metadata": {
        "id": "mhk_9YO9xzOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Simple Model: Regression"
      ],
      "metadata": {
        "id": "BAKWLZmR0sHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Define a dataset"
      ],
      "metadata": {
        "id": "CcNARYmB01bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Generate some sample data\n",
        "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])"
      ],
      "metadata": {
        "id": "rcjWCl_A0w4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the model"
      ],
      "metadata": {
        "id": "aIDAPRox08Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # One input and one output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LinearRegressionModel()\n"
      ],
      "metadata": {
        "id": "o0_jwYLp072K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define the loss function and the optimizer"
      ],
      "metadata": {
        "id": "fpDIe4We1F4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "qThCokhn1LmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training loop\n",
        "\n",
        "write a training loop to iteratively perform the forward pass, compute the loss, perform the backward pass, and update the model parameters, and reset the gradients to zeroes\n"
      ],
      "metadata": {
        "id": "FxSkkRgA1ST4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x_train)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kXCJCar1YFS",
        "outputId": "67fd0dd6-6811-4340-c832-c800acce611a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.0594\n",
            "Epoch [200/1000], Loss: 0.0326\n",
            "Epoch [300/1000], Loss: 0.0179\n",
            "Epoch [400/1000], Loss: 0.0098\n",
            "Epoch [500/1000], Loss: 0.0054\n",
            "Epoch [600/1000], Loss: 0.0030\n",
            "Epoch [700/1000], Loss: 0.0016\n",
            "Epoch [800/1000], Loss: 0.0009\n",
            "Epoch [900/1000], Loss: 0.0005\n",
            "Epoch [1000/1000], Loss: 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Testing the model"
      ],
      "metadata": {
        "id": "1g74PQz21n96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    predictions = model(x_train)\n",
        "    print(\"Predictions:\\n\", predictions)\n",
        "    print(\"Actual:\\n\", y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5beVavRF1rpA",
        "outputId": "b8d5db4c-3c4f-4d98-bd61-8db21cc6ab4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            " tensor([[2.0264],\n",
            "        [4.0128],\n",
            "        [5.9992],\n",
            "        [7.9856]])\n",
            "Actual:\n",
            " tensor([[2.],\n",
            "        [4.],\n",
            "        [6.],\n",
            "        [8.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Training and Evaluation Mode of the model**\n",
        "\n",
        "- model.train(): Sets the model to training mode, enabling behaviors like dropout and batch normalization using the current batch statistics. Gradients are computed, and parameters can be updated.\n",
        "\n",
        "- model.eval(): Sets the model to evaluation mode, disabling dropout and using the running statistics for batch normalization. It prepares the model for inference/testing.\n",
        "\n",
        "- torch.no_grad(): Disables gradient computation to make inference more efficient and reduce memory usage, ensuring that no unnecessary gradients are tracked."
      ],
      "metadata": {
        "id": "tSgqLcU61wOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout** is a regularization technique used in neural networks to prevent overfitting. During training, it randomly \"drops\" (sets to zero) a fraction of neurons in the network at each forward pass, which forces the network to learn more robust and generalizable features."
      ],
      "metadata": {
        "id": "GTPV4qUc2lwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training an Image Classification Model: MNIST Digit Classification"
      ],
      "metadata": {
        "id": "4o9WMI8W29ed"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7ZK4uH23Ecq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}